---
title: NN02.多层感知机
date: 2025-09-17 17:00:49
permalink: /pages/NN_02/
---

# 多层感知机

## 隐藏层

线性模型可能会出错，因此，我们需要在模型中加入隐藏层来增强模型与数据集的拟合。

最简单的方法是将许多的全连接层堆叠在一起，每一层都输出到上面的层，直到生成最后的输出，可以把前 $L -1$ 层看作表示，把最后一层看作线性预测器，这种架构通常称为多层感知机（multiayer perceptron),通常缩写为 MLP，如下图所示：

![mlp](/img/Dp/mlp.svg)

## 激活函数

激活函数（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。

### ReLU函数

_修正线性单元_（Rectified linear unit，_ReLU_）

$$
ReLU(x) = \max(x, 0) \tag{Eq. 3.1}
$$

ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素，图像图下
![output_mlp_76f463_21_0](/img/Dp/output_mlp_76f463_21_0.svg)
当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1

![output_mlp_76f463_36_0](/img/Dp/output_mlp_76f463_36_0.svg)

### sigmoid函数

对于一个定义域在 $\mathbb{R}$ 中的输入， **sigmoid函数**将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为挤压函数（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值：

$$
\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)} \tag{Eq. 3.2}
$$

sigmoid函数在二分类问题上仍被广泛使用，但是在隐藏层中已经较少使用

其图像图下：
![output_mlp_76f463_51_0](/img/Dp/output_mlp_76f463_51_0.svg)
sigmoid函数的导数为下面的公式：

$$
\frac{d}{dx}\text{sigmoid}(x) = \frac{\exp(-x)}{\left(1 + \exp(-x)\right)^2} = \text{sigmoid}(x)\left(1 - \text{sigmoid}(x)\right) \tag{Eq. 3.3}
$$

当输入为 $0$ 时，sigmoid函数的导数达到最大值0.25，但在任意方向越远离 $0$ 时，导数越接近 $0$

![output_mlp_76f463_66_0](/img/Dp/output_mlp_76f463_66_0.svg)

### tanh函数

tanh为双曲正切函数，也能将输入压缩转换到区间 $(-1,1)$ 上，其公式如下：

$$
\tanh(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} \tag{Eq. 3.4}
$$

当输入在 $0$ 附近时，tanh函数接近线性变换，其图像如下：

![output_mlp_76f463_81_0](/img/Dp/output_mlp_76f463_81_0.svg)
tanh的导数为：

$$
\frac{d}{dx}\tanh(x) = 1 - \tanh^2(x) \tag{Eq. 3.5}
$$

当输入接近 $0$ 时，tanh函数的导数皆尽最大值 $1$，输入在任意方向上原理 $0$ 点,导数越接近 $0$

tanh的导数图像如下：

![output_mlp_76f463_96_0](/img/Dp/output_mlp_76f463_96_0.svg)

## 模型选择、欠拟合与过拟合

在训练神经网络时，我们关心的不仅仅是训练集上的表现，而是模型在**新数据上的泛化能力**。  
- **训练误差（training error）**：模型在训练集上的误差。  
- **泛化误差（generalization error）**：模型在测试集或新数据上的误差。  

如果模型太简单，它连训练集都学不好，这叫 **欠拟合（underfitting）**。  
如果模型太复杂，它可能把训练数据中的噪音也记住了，导致测试集效果变差，这叫 **过拟合（overfitting）**。  

常见的应对方法：  
- 欠拟合：增加模型复杂度（更多层、更多隐藏单元）、训练更久、增加特征。  
- 过拟合：加入正则化（如权重衰减、Dropout）、增加数据量或数据增强、提前停止（early stopping）。  

在模型选择过程中，通常会用 **验证集** 或 **交叉验证** 来估计泛化误差，帮助选择更合适的模型。

---

## 权重衰减（Weight Decay / L2 正则化）

**思想**：在损失函数中对模型参数的大小进行惩罚，防止权重过大，从而缓解过拟合。  

修改后的损失函数形式为：

$$
L' = L + \frac{\lambda}{2} \| \mathbf{w} \|_2^2
$$

其中：
- $L$ 是原始损失函数（如均方误差）。  
- $\lambda$ 是正则化强度（超参数）。  
- $|| \mathbf{w} \|_2^2 = \sum_i w_i^2$ 表示权重平方和。  

直观理解：  
- 如果参数过大，就会增加损失，促使优化器把参数控制在较小范围内。  
- 权重衰减可以让模型更平滑，不容易记住训练数据的噪声。  

除了 **L2 正则化**，还有 **L1 正则化**：  

$$
L' = L + \lambda \| \mathbf{w} \|_1
$$

它会使得权重稀疏（很多参数直接变成 0）。  

---

## 暂退法（Dropout）

**思想**：在训练过程中，随机“屏蔽”一部分神经元，让网络在不同的子网络上训练，从而减少过拟合。  

具体做法：  
- 在训练时，以概率 \(p\) 将某些神经元的输出置零。  
- 在测试时，使用完整网络，但将输出按比例缩放，以保证期望一致。  

数学形式（训练阶段）：  

$$
h_i' =
\begin{cases}
0, & \text{以概率 } p \\
\frac{h_i}{1-p}, & \text{以概率 } (1-p)
\end{cases}
$$

其中 $\mathbf{h_i}$ 是神经元原始输出，$\mathbf{h}_{i}^{'}$ 是 Dropout 后的输出。  

直观理解：  
- Dropout 相当于“给模型加噪音”，迫使它不依赖某个特定神经元。  
- 最终效果是减少过拟合，提高泛化能力。  

---

## 4.7 前向传播、反向传播与计算图

### 前向传播（Forward Propagation）
数据从输入层开始，依次通过隐藏层，最后输出预测结果。  
例如单隐层网络：  

$$
\mathbf{h} = \sigma(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1)
$$  
$$
\mathbf{o} = \mathbf{W}_2 \mathbf{h} + \mathbf{b}_2
$$

其中：  
- $\mathbf{x}$ 是输入，$\mathbf{h}$ 是隐藏层输出，$\mathbf{o}$ $是最终输出。  
- $\sigma(\cdot)$ 表示激活函数。  

### 反向传播（Backpropagation）
为了更新参数，我们需要计算损失函数对每一层参数的梯度。  
梯度通过**链式法则**逐层传回：  

$$
\frac{\partial L}{\partial \mathbf{W}_1}
= \frac{\partial L}{\partial \mathbf{h}}
\cdot \frac{\partial \mathbf{h}}{\partial \mathbf{W}_1}
$$

这个过程就是反向传播。  

### 计算图（Computational Graph）
神经网络的计算可以表示为一个有向图：  
- 节点：运算（如加、乘、激活）。  
- 边：张量（数据流）。  

前向传播：数据在图中“前进”。  
反向传播：梯度在图中“反传”。  

深度学习框架（PyTorch、TensorFlow）就是基于计算图自动完成反向传播。  

---

## 4.8 数值稳定性与模型初始化

在训练深层网络时，经常会遇到 **梯度消失** 或 **梯度爆炸** 问题。  

### 梯度消失
- 当使用 Sigmoid/Tanh 等激活函数时，深层传播会让梯度趋近于 0。  
- 结果：前面几层参数几乎不更新。  

### 梯度爆炸
- 如果权重过大或层数过深，梯度在传播过程中会指数级增大。  
- 结果：参数震荡、loss 无法收敛。  

### 解决方法
1. **合适的激活函数**  
   - ReLU 能缓解梯度消失问题。  

2. **参数初始化方法**  
   - **Xavier/Glorot 初始化**（适合 Sigmoid/Tanh）:

     $$
     w \sim U\left(-\sqrt{\frac{6}{n_\text{in} + n_\text{out}}},
                   \sqrt{\frac{6}{n_\text{in} + n_\text{out}}}\right)
     $$ 
     保证前后层输出方差一致。  

   - **He 初始化**（适合 ReLU）：  

     $$
     w \sim \mathcal{N}\left(0, \frac{2}{n_\text{in}}\right)
	  $$

3. **梯度裁剪**（Gradient Clipping）  
   - 如果梯度过大，可以强行限制其大小，避免数值爆炸。  

---

## 环境与分布偏移

### 什么是分布偏移
- 如果训练数据和测试数据分布不同，模型的泛化能力会下降，这叫 **分布偏移（distribution shift）**。  

### 类型
1. **协变量偏移（Covariate Shift）**  
   - 输入分布变了，但条件分布 \(P(y|x)\) 不变。  
   - 例如：训练图片很清晰，测试图片有噪声。  

2. **标签偏移（Label Shift）**  
   - 标签的分布变了。  
   - 例如：训练集中猫狗各 50%，测试集里猫占 90%。  

3. **概念偏移（Concept Shift）**  
   - 输入和输出的映射关系变了。  
   - 例如：医学诊断标准更新。  

### 应对方法
- 数据增强，让训练数据更接近真实场景。  
- 收集更多与实际环境匹配的数据。  
- 迁移学习，把在一个任务上学到的知识迁移到另一个任务上。  

---
