---
title: 01.线性回归
date: 2025-09-08 21:00:00
permalink: /pages/DP_01/
---

回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

## 线性回归的基本原理
线性回归是最早的一类回归模型，其基于几个简单的假设：
+ 假设自变量和因变量之间的关系是线性的， 即 $\mathbf{y}$ 可以表示为 $\mathbf{x}$ 中元素的加权和，这里通常允许包含观测值的一些噪声
+ 任何噪声都比较正常，如遵循正态分布

---

下面阐述一些基本概念：
我们为了开发一个模型，首先要做的就是收集一个真实的数据集。
在机器学习的术语中，
+ 该数据集被称为`训练数据集`（training data set)
+ 每行数据为`样本`（sample）
+ 试图预测的目标称为`标签`（label）
+ 预测所需要的自变量称为`特征`（feature）

通常，我们使用 $n$ 来表示数据集中的样本数。对索引为 $i$ 的样本，其输入表示为

$$
\mathbf{x}^{(i)} = \left[ x_1^{(i)}, x_2^{(i)} \right]^{\top}
$$
对应的标签是 $\mathbf{y}^{(i)}$.

---

## 线性模型
如下面的公式：
$$
\text{price} = w_{\text{area}} \cdot \text{area} + w_{\text{age}} \cdot \text{age} + b
$$

其中的 $w_{\text{area}}$ 和 $w_{\text{age}}$ 称为权重（weight），权重决定了每个特征对我们预测值的影响，$\mathbf{b}$ 称为偏置（bias），偏执是指当所有特征都去值为0时，预测值应该为多少。

在机器学习领域，使用的通常是高维数据集，建模时采用线性代数表示法，下面的三个公式展示了预测结果（$\hat{y}$) 与特征的关系，维度依次上升，不过多阐述：
$$
\hat{y} = w_1x_1 + \dots + w_dx_d + b
$$

$x_1, x_2, \dots, x_d$（单个数据样本的$d$个特征)
$$
\hat{y} = \mathbf{w}^\top \mathbf{x} + b
$$
$\mathbf{x}$（单个数据样本的特征向量，包含$d$个特征，$\mathbf{x} \in \mathbb{R}^d$）
$$
\hat{\mathbf{y}} = \mathbf{X}\mathbf{w} + b
$$

$\mathbf{X}$（整个数据集的特征矩阵，$\mathbf{X} \in \mathbb{R}^{n \times d}$，其中$n$是样本数，每一行对应一个样本的$d$个特征）

---

## 损失函数
在拟合（fit）数据之前，需要确定一个拟合程度的度量，而损失函数（loss function)正好能够量化目标的实际值与预测值之间的差距，通常选择非负数，数值越小表示损失越小，完美时的损失为0.

常用的损失函数是**平方误差函数**，当样本的预测值为 $\hat{y}^{(i)}$,其相应真实标签为 $y^{(i)}$时，平方误差可以定义为一下公式：
$$
l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left( \hat{y}^{(i)} - y^{(i)} \right)^2
$$
由于二次项的固有特性，较大的差异将会导致更大的损失，因此，为了度量模型在整个数据集上的质量，我们需要在训练集 $n$ 个样本上的损失值（等价于求和）
$$
L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \left( \mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)} \right)^2
$$


训练模型时，我们最希望的是找一组参数 $(\mathbf{w}^*, b^*)$，这组参数能够最小化在所有训练样本上的总损失：
$$
\mathbf{w}^*, b^* = \underset{\mathbf{w}, b}{\operatorname{argmin}} \ L(\mathbf{w}, b)
$$

## 随机梯度下降
梯度下降（gradient descent) 的方法几乎可以优化所有深度学习模型，其通过不断在损失函数递减的方向上更新参数来降低误差。

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做**小批量随机梯度下降**（minibatch stochastic gradient descent）。

在每次迭代中，我们首先随机抽样一个小批量$\mathcal{B}$， 它是由固定数量的训练样本组成的。然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数 $\eta$，并从当前参数的值中减掉。

下面的数学公式可以表示这一更新过程（$\partial$表示偏导数）：
$$
(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b)
$$
