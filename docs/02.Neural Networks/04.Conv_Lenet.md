---
title: NN 04.卷积和LeNet神经网络
date: 2026-02-01 12:00:00
permalink: /pages/NN_04/
---

本章用于记录我在学习卷积神经网络（CNN）过程中，
对卷积相关概念以及初代经典 CNN —— **LeNet** 的结构理解。

## 一、卷积相关知识点

### 1. 卷积（Convolution）

在深度学习中，卷积运算本质上更接近于**互相关（cross-correlation）**。

可以形象地理解为：  
一个较小的卷积核（kernel）在一张较大的输入特征图上，从左到右、从上到下滑动，
在每一个位置与局部区域进行加权求和，从而生成新的**特征图（feature map）**。

---

### 2. 填充（Padding）

填充是指在输入特征图的边缘补充若干行或列的像素。

其主要目的包括：

- 控制输出特征图的空间尺寸
- 保留输入图像边缘的信息

通常用 $P$ 表示填充的像素数。

---

### 3. 步幅（Stride）

步幅表示卷积核在输入特征图上**每次滑动的像素数**，通常记为 $S$。

- $S = 1$：逐像素滑动（最常见）
- $S > 1$：下采样效果，输出尺寸减小

---

### 4. 神经网络中的张量表示

在卷积神经网络中，输入数据通常表示为一个 **四维张量**：

$$
\texttt{(batch\_size,\ channels,\ height,\ width)}
$$

各个维度的含义如下：

- **batch_size**：批量大小，一次输入网络的样本数量  
- **channels**：通道数（如灰度图为 1，RGB 图像为 3）  
- **height**：特征图高度  
- **width**：特征图宽度  

---

### 5. 池化（Pooling）

池化操作与卷积类似，同样在局部区域内进行运算，但有以下特点：

- **不改变通道数**
- 主要作用是减小特征图的空间尺寸
- 提升平移不变性，降低计算量

常见的池化方式包括平均池化（Avg Pooling）和最大池化（Max Pooling）。

---

### 6. 卷积与池化的输出尺寸计算

对于卷积或池化操作，其输出特征图的空间尺寸计算公式为：

$$
\text{output\_size}
=
\left\lfloor
\frac{\text{input\_size} - K + 2P}{S}
\right\rfloor
+ 1
$$

其中：

- **input_size**：输入特征图的高度或宽度  
- **K**：卷积核（或池化核）的尺寸  
- **P**：填充大小（padding）  
- **S**：步幅（stride）  
- $\lfloor \cdot \rfloor$：向下取整  

---

## 二、LeNet 网络结构

下面使用 PyTorch 代码的形式，对经典 CNN —— **LeNet** 的整体结构进行说明。

```python
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5, padding=2),
    # 第一层卷积
    # 输入通道数为 1（灰度图），输出通道数为 6
    # padding=2，使输出特征图尺寸保持为 28×28

    nn.Sigmoid(),
    # 激活函数
    # 将卷积结果映射到 (0, 1) 区间

    nn.AvgPool2d(kernel_size=2, stride=2),
    # 平均池化层
    # 不改变通道数，仅将特征图宽高减半：28 -> 14

    nn.Conv2d(6, 16, kernel_size=5),
    # 第二层卷积
    # 输入通道数为 6，输出通道数为 16
    # 特征图尺寸：14 -> 10

    nn.Sigmoid(),
    # 激活函数

    nn.AvgPool2d(kernel_size=2, stride=2),
    # 平均池化层
    # 特征图尺寸：10 -> 5

    nn.Flatten(),
    # 展平层
    # 将张量从 (16, 5, 5) 展平成 (400,)

    nn.Linear(400, 120),
    # 全连接层
    # 输入维度 400，输出维度 120

    nn.Sigmoid(),
    # 激活函数

    nn.Linear(120, 84),
    # 全连接层
    # 通道数：120 -> 84

    nn.Sigmoid(),
    # 激活函数

    nn.Linear(84, 10)
    # 输出层
    # 输出维度等于类别数
)