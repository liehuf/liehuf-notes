---
title: NN 06.并行块和深度神经网络的深究解释
date: 2026-02-01 14:00:00
permalink: /pages/NN_06/
---

本篇博客着重于解释自己对于 GoogLeNet 以及深层网络 ResNet、DenseNet 的理解。

## 一、GoogLeNet 网络并行化的阐述

GoogLeNet 是一个巨大的进展，它的设计思想是从单纯追求网络深度，转向同时强调网络的宽度。

一个 GoogLeNet 的 Inception 块由四条并行线路组成，它们共享相同的输入通道数，并在输出端进行通道维拼接（concatenate）。

四条路径如下：

**路径一：**

Conv 1×1  
作用：保留空间分辨率，调整通道数，输出结果参与最终拼接。

**路径二：**

Conv 1×1（降维）  
→ Conv 3×3（padding=1）  
作用：在保证空间分辨率不变的前提下，引入中等感受野。

**路径三：**

Conv 1×1（降维）  
→ Conv 5×5（padding=2）  
作用：引入更大的感受野，捕捉更大尺度特征。

**路径四：**

MaxPool（kernel=3，stride=1，padding=1）  
→ Conv 1×1  
作用：引入池化特征，同时通过 1×1 卷积控制通道数。

并行结构的核心意义在于：  
在同一层级同时观察不同尺度的特征，并将它们在通道维度上整合。

## 二、批量规范化（Batch Normalization）

批量规范化是一种在训练神经网络时用于稳定并加速训练过程的技术。

在训练深层网络时，随着网络层数的加深，前面层参数的变化会不断改变后续层接收到的数据分布，这一现象被称为内部协变量转移（Internal Covariate Shift）。

Batch Normalization 的目标是：  
使每一层的输入保持相对稳定的分布，避免激活值过大或过小，从而使梯度下降过程更加稳定、更易收敛。

在每一个小批量（mini-batch）数据送入网络时，BatchNorm 对当前层的每个通道（或神经元）进行两步处理。

第一步：对激活值做标准化（Normalization），计算当前 batch 的均值和标准差，将数据规范化为均值接近 0、标准差接近 1 的分布：

$$
\hat{x} = \frac{x - \mu}{\sigma}
$$

第二步：引入可学习的缩放与平移参数：

$$
y = \gamma \hat{x} + \beta
$$

其中 $\gamma$ 与 $\beta$ 为可学习参数，使网络能够学习最合适的尺度与偏移，而非被固定为标准正态分布。

Batch Normalization 的意义在于：  
加快训练速度（学习率可以调大）、降低对权重初始化的敏感性，并在一定程度上起到正则化作用（均值与方差存在噪声）。

在使用习惯上，BatchNorm 一般放在激活函数之前；其参数 $\gamma$、$\beta$ 会参与反向传播；训练模式下使用当前 batch 的均值与方差，推理模式下使用全局移动平均估计值。

## 三、ResNet 的基本思想

ResNet 的核心思想是残差学习，即让网络块学习输入和输出之间的“差值”，而不是直接学习一个新的映射。

其基本形式为：

$$
Y = X + F(X)
$$

一个基本残差块可描述为：

```text
X
│
├─ Conv 3×3 → BN → ReLU → Conv 3×3 → BN ──┐
│                                         │
└──────────── Conv 1×1（可选）────────────┘
                      ↓
                   相加
                      ↓
                    ReLU
                      ↓
                      Y

```

当输入与输出的通道数不一致时，可在捷径分支上使用 Conv 1×1 进行匹配。

残差结构的核心作用在于缓解深层网络的退化问题，使梯度更容易在深层网络中传播。

## 四、DenseNet 的研究解释

DenseNet 的设计思想是：每一层都直接连接到之前所有层的特征图（feature map）。

DenseNet 使用的是通道维拼接（concatenate），而非 ResNet 中的逐元素相加。

其形式可表示为：

$$
[X, f_1(X), f_2([X, f_1(X)]), f_3([X, f_1(X), f_2(\cdot)])]
$$

### 稠密块（Dense Block）

一个最小计算单元由以下结构组成：

BN → ReLU → Conv 3×3（padding=1）

其输出通道数固定。

核心计算逻辑如下：

```python
for i in range(num_convs):
    layer.append(
        conv_block(num_channels * i + input_channels, num_channels)
    )
```

其中，`num_channels` 为增长率（growth rate）。
 第 $i$ 个卷积块的输入通道数等于原始输入通道数加上前 $i$ 个卷积块所产生的新通道数。

即：前面所有层的输出，都会作为后面所有层的输入。

### 过渡层（Transition Layer）

由于稠密连接会导致通道数快速增长，从而引发计算量和参数规模的膨胀，因此需要在稠密块之间引入过渡层进行控制。

过渡层结构为：

BN → ReLU → Conv 1×1 → AvgPool2d（stride=2）

其作用是压缩通道数并减小特征图的空间尺寸。

### DenseNet 整体架构

初始层：Conv → BN → ReLU → MaxPool，用于提取初始特征并降低分辨率。

稠密模块（Dense Block）：多层稠密连接，持续积累特征表示。

过渡层（Transition Layer）：控制通道数量并减小特征图尺寸。

输出阶段（分类部分）：BN → ReLU → 全局平均池化 → 全连接层 → 分类输出。

