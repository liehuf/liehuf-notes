---
title: NN 01.关于线性网络的研究和记录
date: 2026-01-31 22:00:00
permalink: /pages/NN_01/
---

本章节用于记录我在学习《动手学深度学习》（D2L）过程中，对线性神经网络相关内容的理解与整理。  
重点不在复述教材，而在于理清一些容易被忽略或想当然的基础概念。

## 一、前置的基本概念

### 1. 张量（Tensor）

张量可以理解为由数值组成的多维数组。

- 一维张量称为向量（vector）
- 二维张量称为矩阵（matrix）
- 更高维的统称为张量（tensor）

在深度学习中，张量是数据、参数以及梯度的统一表示形式。

### 2. 向量的方向约定

在数学与深度学习中，向量默认采用列向量形式，例如：

$$
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
\in \mathbb{R}^{2}
$$

这一约定会直接影响矩阵乘法是否合法，也是后续公式推导的基础。

### 3. 范数（Norm）

范数用于衡量向量或矩阵的大小。

向量的 L2 范数定义为：

$$
\lVert \mathbf{x} \rVert_2
=
\sqrt{\sum_{i=1}^{n} x_i^2}
$$

矩阵的 L2 范数（Frobenius 范数）定义为：

$$
\lVert \mathbf{X} \rVert_F
=
\sqrt{\sum_{i}\sum_{j} X_{ij}^2}
$$

### 4. 自动微分（Automatic Differentiation）

自动微分是深度学习框架提供的一种机制，用于自动计算梯度。  
它可以避免手动求导带来的错误，并且在复杂网络中依然保持高效。

### 5. 计算图（Computation Graph）

计算图是框架在执行正向与反向传播时构建的一种有向图结构。

- 节点表示变量或中间结果  
- 边表示算术运算关系  

### 6. 反向传播（Backpropagation）

反向传播利用微积分中的链式法则，根据损失函数对参数的梯度来更新模型参数。  
在张量层面，本质上是梯度在计算图中的逐层传递。

## 二、随机梯度下降

在实际训练中，通常采用小批量随机梯度下降（Mini-batch SGD）。

参数更新公式为：

$$
(\mathbf{w}, b)
\leftarrow
(\mathbf{w}, b)
-
\frac{\eta}{|\mathcal{B}|}
\sum_{i \in \mathcal{B}}
\nabla_{(\mathbf{w}, b)} \, \ell^{(i)}
$$

其中：

- $\mathcal{B}$ 表示一个小批量样本集合  
- $|\mathcal{B}|$ 表示批量大小（batch size）  
- $\eta$ 表示学习率（learning rate）  
- $\ell^{(i)}$ 表示第 $i$ 个样本对应的损失  

## 三、线性回归模型

设数据集中共有 $n$ 个样本，对于第 $i$ 个样本，其输入向量表示为：

$$
\mathbf{x}^{(i)} =
\begin{bmatrix}
x_1^{(i)} \\
x_2^{(i)}
\end{bmatrix}
\in \mathbb{R}^{2}
$$

对应的标签为标量 $y^{(i)}$。

模型参数定义为：

权重向量：

$$
\mathbf{w} =
\begin{bmatrix}
w_1 \\
w_2
\end{bmatrix}
\in \mathbb{R}^{2}
$$

偏置为标量 $b$。

线性回归模型的预测形式为：

$$
\hat{y}^{(i)} = \mathbf{w}^\top \mathbf{x}^{(i)} + b
$$

## 四、损失函数

### 1. 平方损失函数

单个样本的平方损失定义为：

$$
\ell^{(i)}(\mathbf{w}, b)
=
\frac{1}{2}
\left(
\hat{y}^{(i)} - y^{(i)}
\right)^2
$$

### 2. 总体损失函数

整体损失函数定义为：

$$
L(\mathbf{w}, b)
=
\frac{1}{n}
\sum_{i=1}^{n}
\ell^{(i)}(\mathbf{w}, b)
$$

训练的目标是找到一组参数 $(\mathbf{w}, b)$，使损失函数 $L(\mathbf{w}, b)$ 最小。
