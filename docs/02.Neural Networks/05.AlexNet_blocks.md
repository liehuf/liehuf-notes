---
title: NN 05.AlexNet 与使用“块”的神经网络
date: 2026-02-01 13:00:00
permalink: /pages/NN_05/
---

本博客为研究阐述型博客，不具备教学性质，仅用于记录个人在学习卷积神经网络过程中的理解与思考。

## 一、AlexNet 网络原理研究

AlexNet 可以视为 LeNet 的进阶版本。

一方面，它继承了 LeNet 中「卷积 + 池化 + 全连接」的整体架构思想；  
另一方面，它在**非线性建模能力与网络深度**方面进行了更为激进的尝试。

---

### 1. 网络块的雏形

AlexNet 在结构设计上，最显著的特点之一是开始**显式使用“块（block）”的思想**。

网络最前端采用了两个连续的：

> 卷积层 + 激活函数 + 最大池化层

这样的组合形式，这已经非常接近后续网络中常见的“网络块”概念。

- 第一个网络块使用 **11×11** 的大卷积核  
  - 目的：快速扩大感受野  
  - 构建「低分辨率、高通道数」的特征表示  

- 第二个网络块在此基础上进一步扩大通道数  
  - 提升特征表达能力  

---

### 2. 3×3 卷积的连续堆叠

在前两层块之后，AlexNet 使用了三层 **卷积核大小为 3×3 的卷积层**，并在其后接入最大池化层。

以通道数变化为例，可以看到其设计上的巧妙之处：

- **256 → 384**  
  将已有特征映射到更高维空间中，增强组合能力  

- **384 → 384**  
  在保持通道数不变的情况下进行特征重组，加强非线性表达  

- **384 → 256**  
  在压缩通道数的同时，获得更强的非线性表达能力与更大的有效感受野  

这种“扩展—重组—压缩”的通道设计，在后续网络中被大量沿用。

---

### 3. 全连接层与丢弃层

在卷积特征提取完成后，AlexNet 使用展平层（Flatten）将特征图转换为向量。

随后，网络采用：

- 两个 **“全连接层 + 激活函数 + Dropout（丢弃率 0.5）”**
- 最后一层全连接层作为输出层

其中，引入 **Dropout** 的主要目的在于：

> 有效缓解深层网络在大规模参数下的过拟合问题。

---

## 二、VGG11 网络架构研究

VGG11 属于 VGG 系列模型之一。

其中的 “11” 表示：

> 8 个卷积层 + 3 个全连接层 = 11 个具有可训练参数的层  
> （池化层不计入）

---

### 1. VGG11 的核心设计思想

VGG11 的核心特点可以概括为：

- 统一使用 **3×3 卷积核**
- 卷积层后统一接 **ReLU 激活函数**
- 使用最大池化进行下采样
- 使用全连接层完成分类任务

本质上，VGG 系列是对 AlexNet 中「块思想」的系统化与规范化。

---

### 2. 一个典型的 VGG 块

下面以 VGG11 中的一个典型卷积块为例说明其结构：

|  层类型   |            参数说明            |
| :-------: | :----------------------------: |
|  Conv2d   | kernel size = 3×3, padding = 1 |
|   ReLU    |           非线性激活           |
| MaxPool2d |  kernel size = 2, stride = 2   |

该结构的特点是：

- 通过 padding 保持卷积前后的空间尺寸
- 通过池化在块末尾进行统一下采样
- 结构简单但可重复堆叠

---

## 三、网络中的网络：NiN（Network in Network）

NiN 网络在继承 VGG 系列**块结构思想**的基础上，
对网络的后半部分结构进行了极具突破性的改进。

其核心创新在于：

> 用 **全局平均池化（Global Average Pooling, GAP）**
> 替代传统的全连接层。

---

### 1. NiN 块的定义

一个典型的 NiN 块由如下结构组成：

|      层结构      |                           功能说明                           |
| :--------------: | :----------------------------------------------------------: |
| Conv(k×k) → ReLU |                       提取局部空间特征                       |
| Conv(1×1) → ReLU | 对每个像素位置执行一次带 ReLU 的 MLP 隐层，增强通道间的非线性组合能力 |
| Conv(1×1) → ReLU |                    进一步提升局部表达能力                    |

其中，1×1 卷积可以被理解为：

> 在每一个像素位置上，对通道维度执行一次多层感知机（MLP）。

---

### 2. 全局平均池化（GAP）

在网络的最后，NiN 使用全局平均池化层代替全连接层。

对于第 $c$ 个通道，其 GAP 定义为：

$$
\text{GAP}_c
=
\frac{1}{H \times W}
\sum_{i=1}^{H}
\sum_{j=1}^{W}
X_{c,i,j}
$$

其中：

- $X_{c,i,j}$ 表示第 $c$ 个通道在空间位置 $(i,j)$ 的激活值  
- $H, W$ 为特征图的高度和宽度  

---

### 3. 直观理解

通过 GAP：

- 每一个通道最终被压缩为一个标量
- 每一个通道可以被视为对应一个类别的“响应图”

因此，GAP 的含义可以理解为：

> 在整个图像范围内，该类别相关特征的**整体证据是否足够强**。

这种设计显著减少了参数量，也提升了模型的泛化能力。

