---
title: NN 03.若干激活函数的研究
date: 2026-01-31 22:30:00
permalink: /pages/NN_03/
---

本文用于记录我在训练神经网络过程中，对常用激活函数的理解。
本篇聚焦于目前使用最广泛的一种激活函数 —— ReLU。

## 一、ReLU 激活函数

### 1. 基本定义

ReLU 的全称是 **线性修正单元（Rectified Linear Unit）**。

其数学定义为：

$$
\text{ReLU}(x) = \max(0, x)
$$

从几何角度看，在二维平面直角坐标系中：

- 当 $x \le 0$ 时，函数输出恒为 0  
- 当 $x > 0$ 时，函数表现为一条斜率为 1 的直线 $y = x$

也可以理解为：  
**y 轴左侧被“压扁”为 0，右侧保持线性增长。**

---

### 2. 梯度性质

ReLU 在不同区间上的导数为：

$$
\frac{d}{dx}\,\text{ReLU}(x) =
\begin{cases}
1, & x > 0 \\
0, & x \le 0
\end{cases}
$$

这一性质带来两个直接结果：

- 当 $x > 0$ 时，梯度恒为 1  
  → 不会出现梯度消失  
- 梯度不会大于 1  
  → 不容易引发梯度爆炸  

因此，在深层网络中，ReLU 相比 sigmoid 或 tanh 更容易训练。

---

### 3. 反向传播中的作用

在反向传播过程中，ReLU 的梯度行为非常简单：

- 若前向传播时该神经元输出为正，则梯度可以正常向后传递  
- 若前向传播时输出为 0，则梯度被直接截断  

从实现角度看，ReLU 本质上只是一个**逐元素的条件判断操作**。

---

### 4. ReLU 的线性与非线性

虽然 ReLU 在每个区间内都是线性的，但它在整体上是一个**非线性函数**。

其非线性体现在：

> 网络对输入空间进行了基于条件的分段线性映射。

不同神经元在不同输入条件下被激活，使得整个网络可以拟合复杂的非线性关系。

---

### 5. “激活”的直观含义

从更直观的角度理解，ReLU 的“激活”可以看作一种**信息门控机制**：

- 输出为 0：该神经元在当前输入下**不参与计算**
- 输出大于 0：该神经元被**激活并参与后续计算**

因此，ReLU 等价于让神经元在“参与计算 / 不参与计算”之间做出选择。

---







