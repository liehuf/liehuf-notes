---
title: NN 02.若干损失函数的研究
date: 2026-01-31 22:00:00
permalink: /pages/NN_02/
---



---
title: NN 02. 损失函数记录：交叉熵损失
date: 2026-01-31 23:00:00
permalink: /pages/NN_02/a
---

本博客用于记录我在学习《动手学深度学习》（D2L）过程中，对若干常见损失函数的理解。
本篇先从分类问题中最常用的 **交叉熵损失函数** 开始。

## 一、交叉熵损失函数

### 1. 本质理解

交叉熵损失的本质是：

> 衡量模型预测得到的**概率分布**与真实标签对应的**理想分布**之间的差异。

在分类问题中，模型并不是直接输出“类别”，而是输出对每个类别的置信程度（概率）。

---

### 2. Softmax 函数

设模型对某一个样本的输出为一个向量：

$$
\mathbf{z} = [z_1, z_2, \dots, z_h]^\top
$$

其中 $h$ 表示类别数，$\mathbf{z}$ 中的每一个分量称为 logit。

Softmax 函数将该向量映射为一个概率分布：

$$
p_i = \frac{\exp(z_i)}{\sum_{j=1}^{h} \exp(z_j)}
$$

Softmax 输出满足以下性质：

- $p_i \ge 0$
- $\sum_{i=1}^{h} p_i = 1$

因此，$\mathbf{p} = [p_1, p_2, \dots, p_h]^\top$ 可以被解释为模型对各个类别的预测概率。

---

### 3. 真实标签的表示方式

假设某个样本的真实类别为 $y$，通常将其表示为 one-hot 向量：

$$
\mathbf{y}_{\text{one-hot}} =
[0, \dots, 1, \dots, 0]
$$

其中第 $y$ 个位置为 1，其余位置为 0。

这种表示方式对应一个“理想”的概率分布：  
模型应当对真实类别给出概率 1，对其他类别给出概率 0。

---

### 4. 交叉熵损失的定义

对于单个样本，交叉熵损失定义为：

$$
L = -\log(p_y)
$$

其中 $p_y$ 表示模型对真实类别 $y$ 给出的预测概率。

该公式意味着：

- 如果模型对真实类别预测得越自信（$p_y$ 越大），损失越小
- 如果模型对真实类别预测得越不确定（$p_y$ 越小），损失会迅速增大

---

### 5. 数值示例

- 当 $p_y = 0.9$ 时：

$$
L = -\log(0.9) \approx 0.105
$$

损失较小，说明模型预测效果较好。

- 当 $p_y = 0.1$ 时：

$$
L = -\log(0.1) \approx 2.302
$$

损失显著增大，说明模型对真实类别的判断非常不自信。  
哦，玛卡巴卡！

---

### 6. 一个直观理解

交叉熵损失实际上只关心一件事：

> **模型是否把足够高的概率分配给了真实类别。**

至于其他类别的概率如何分配，只会通过归一化间接影响这一结果。
