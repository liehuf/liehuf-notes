---
title: 02.SoftMax回归
date: 2025-09-17 17:00:49
permalink: /pages/DP_02/
---

# SoftMax 回归

## 分类问题

独热编码（one-hot encodeing):一个向量，其分量和类别一样多。类别对应的分量为1，其他分量为0. 

举一个最简单的例子，我们分类各种瓜，标签 $y$ 将会是一个三维向量，其中$\vec (1,0,0)$对应“冬瓜”，$(0,1,0)$对应“西瓜”，$(0,0,1)$对应“傻瓜”，

$$
y \in \{(1,0,0),(0,1,0),(0,0,1)\tag {Eq. 2.1}
$$

---

## 网络架构

softmax回归是一个单层神经网络，同时也是全连接层，如下图，计算输出$o_1$、$o_2$和$o3$取决于所有输入$x_1$、$x_2$、$x_3$、$x_4$.

![softmaxreg](/img/Dp/softmaxreg.svg)

## softmax运算

softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质，如下式：

$$
\hat{\mathbf{y}} = \text{softmax}(\mathbf{o}) \quad \text{其中} \quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)} \tag{Eq. 2.2}
$$

上式中对于所有的$j$总会有$0 \leq \hat{y}_j \leq 1$，因此，$\hat{y}$ 是一个正确的概率分布。

## 小批量样本的矢量化

可以假设我们读取了一个批量的样本 $\mathbf{X}$,其中特征维度（输入数量）为$d$, 批量大小为$n$, 输出中有$q$个类别。则小批量样本的特征为$\mathbf{X} \in \mathbb{R}^{n \times d}$，权重为$\mathbf{W} \in \mathbb{R}^{d \times q}$,偏置为$\mathbf{b} \in \mathbb{R}^{1 \times q}$,softmax的矢量计算表达式为：

$$ 
\begin{aligned} \mathbf{O} &= \mathbf{XW} + \mathbf{b}, \\ \hat{\mathbf{Y}} &= \text{softmax}(\mathbf{O}). \end{aligned} \tag{Eq. 2,3}
$$

## 交叉熵损失函数

softmax给出了一个向量$\hat {y}$, 通俗地说，它就是一个条件概率，表示给定任意输入$x$的每个类的条件概率，举个例子：

$$
\hat {y}_3 = P(y = \text{傻瓜} \mid \mathbf{x}) \tag{Eq. 2.4}
$$

根据最大似然估计法（中间省略很多内容），就可以得到下面的损失函数，对于标签$y$和模型预测$\hat {y}$,其表达式为：

$$ 
l(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{j=1}^{q} y_j \log \hat{y}_j \tag {Eq. 2.5}
$$

## softmax的导数

将 `Eq. 2.2` 代入 `Eq. 2.5` ，同时根据softmax和独热编码的定义，我们得到：

$$ 
\begin{aligned} l(\mathbf{y}, \hat{\mathbf{y}}) &= -\sum_{j=1}^{q} y_j \log \frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)} \\ &= \sum_{j=1}^{q} y_j \log \sum_{k=1}^{q} \exp(o_k) - \sum_{j=1}^{q} y_j o_j \\ &= \log \sum_{k=1}^{q} \exp(o_k) - \sum_{j=1}^{q} y_j o_j \end{aligned} \tag {Eq. 2.6}
$$

接下来是softmax的导数，相较于任何未规范化的预测$o_j$的导数：

$$ 
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^{q} \exp(o_k)} - y_j = \text{softmax}(\mathbf{o})_j - y_j \tag {Eq. 2.7}
$$

