<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>NN 04.卷积和LeNet神经网络 | 原码纪事</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/img/base/favicon.ico">
    <script language="javascript" type="text/javascript" src="/js/pgmanor-self.js"></script>
    <script async="true" src="https://www.googletagmanager.com/gtag/js?id=G-LPRG9SPLFF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-LPRG9SPLFF');
    </script>
    <meta name="description" content="vdoing博客主题模板">
    <meta name="keywords" content="猎户f,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/assets/css/0.styles.e44029f7.css" as="style"><link rel="preload" href="/assets/js/app.542ebbc9.js" as="script"><link rel="preload" href="/assets/js/2.e2bf2c87.js" as="script"><link rel="preload" href="/assets/js/15.350f58ce.js" as="script"><link rel="prefetch" href="/assets/js/10.0302174d.js"><link rel="prefetch" href="/assets/js/11.107b05d0.js"><link rel="prefetch" href="/assets/js/12.6dd44963.js"><link rel="prefetch" href="/assets/js/13.cadc9fd8.js"><link rel="prefetch" href="/assets/js/14.20aa7c6d.js"><link rel="prefetch" href="/assets/js/16.05e6aae6.js"><link rel="prefetch" href="/assets/js/17.9bcfd6f2.js"><link rel="prefetch" href="/assets/js/18.eca2e313.js"><link rel="prefetch" href="/assets/js/19.032901b0.js"><link rel="prefetch" href="/assets/js/20.c3a93049.js"><link rel="prefetch" href="/assets/js/21.2a6da497.js"><link rel="prefetch" href="/assets/js/22.7a3b2bc4.js"><link rel="prefetch" href="/assets/js/23.5300cb21.js"><link rel="prefetch" href="/assets/js/24.08b3e4c6.js"><link rel="prefetch" href="/assets/js/25.b48f20f6.js"><link rel="prefetch" href="/assets/js/26.66856a58.js"><link rel="prefetch" href="/assets/js/27.20bd8a53.js"><link rel="prefetch" href="/assets/js/28.14551ea5.js"><link rel="prefetch" href="/assets/js/29.b119bcd2.js"><link rel="prefetch" href="/assets/js/3.32e4edb6.js"><link rel="prefetch" href="/assets/js/30.4e9f47a0.js"><link rel="prefetch" href="/assets/js/31.7d397ed9.js"><link rel="prefetch" href="/assets/js/32.effa0449.js"><link rel="prefetch" href="/assets/js/33.33b6180d.js"><link rel="prefetch" href="/assets/js/34.c602cf87.js"><link rel="prefetch" href="/assets/js/35.d5095fdc.js"><link rel="prefetch" href="/assets/js/36.1b3a9997.js"><link rel="prefetch" href="/assets/js/37.2254cddb.js"><link rel="prefetch" href="/assets/js/38.8a29293b.js"><link rel="prefetch" href="/assets/js/39.48f98270.js"><link rel="prefetch" href="/assets/js/4.a8f5cda8.js"><link rel="prefetch" href="/assets/js/40.db4d8e41.js"><link rel="prefetch" href="/assets/js/41.f8e8829f.js"><link rel="prefetch" href="/assets/js/42.8631f79f.js"><link rel="prefetch" href="/assets/js/43.d05f1366.js"><link rel="prefetch" href="/assets/js/44.bc5d9beb.js"><link rel="prefetch" href="/assets/js/45.f4b9e488.js"><link rel="prefetch" href="/assets/js/46.d0a7da03.js"><link rel="prefetch" href="/assets/js/47.a0f27ec9.js"><link rel="prefetch" href="/assets/js/48.acb70c49.js"><link rel="prefetch" href="/assets/js/49.c395dd91.js"><link rel="prefetch" href="/assets/js/5.f1f6522d.js"><link rel="prefetch" href="/assets/js/50.a4748cba.js"><link rel="prefetch" href="/assets/js/51.73443236.js"><link rel="prefetch" href="/assets/js/52.1b3738ff.js"><link rel="prefetch" href="/assets/js/53.839d2abc.js"><link rel="prefetch" href="/assets/js/54.5d3c4a18.js"><link rel="prefetch" href="/assets/js/55.2f61f03f.js"><link rel="prefetch" href="/assets/js/56.7612b0a8.js"><link rel="prefetch" href="/assets/js/57.58cdca50.js"><link rel="prefetch" href="/assets/js/58.1fa304c8.js"><link rel="prefetch" href="/assets/js/59.9888d371.js"><link rel="prefetch" href="/assets/js/6.94bf3be4.js"><link rel="prefetch" href="/assets/js/60.149741c1.js"><link rel="prefetch" href="/assets/js/61.bfd5776e.js"><link rel="prefetch" href="/assets/js/62.157f863f.js"><link rel="prefetch" href="/assets/js/63.04df2cde.js"><link rel="prefetch" href="/assets/js/64.d0f34c68.js"><link rel="prefetch" href="/assets/js/65.d55ce17b.js"><link rel="prefetch" href="/assets/js/66.91e974d3.js"><link rel="prefetch" href="/assets/js/7.6e92e50d.js"><link rel="prefetch" href="/assets/js/8.d2e18a7e.js"><link rel="prefetch" href="/assets/js/9.9c0da181.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e44029f7.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/base/bar.png" alt="原码纪事" class="logo"> <span class="site-name can-hide">原码纪事</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/pages/Notes/" class="nav-link">博客笔记</a></div><div class="nav-item"><a href="/pages/Projects/" class="nav-link">工程</a></div><div class="nav-item"><a href="/pages/Teasting/" class="nav-link">吐槽</a></div><div class="nav-item"><a href="/message-board/" class="nav-link">留言板</a></div><div class="nav-item"><a href="https://blog.yuanmajishi.top/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  我的博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/pages/Notes/" class="nav-link">博客笔记</a></div><div class="nav-item"><a href="/pages/Projects/" class="nav-link">工程</a></div><div class="nav-item"><a href="/pages/Teasting/" class="nav-link">吐槽</a></div><div class="nav-item"><a href="/message-board/" class="nav-link">留言板</a></div><div class="nav-item"><a href="https://blog.yuanmajishi.top/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  我的博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/pages/NN_01/" class="sidebar-link">NN 01.关于线性网络的研究和记录</a></li><li><a href="/pages/NN_02/" class="sidebar-link">NN 02.若干损失函数的研究</a></li><li><a href="/pages/NN_03/" class="sidebar-link">NN 03.若干激活函数的研究</a></li><li><a href="/pages/NN_04/" aria-current="page" class="active sidebar-link">NN 04.卷积和LeNet神经网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/NN_04/#一、卷积相关知识点" class="sidebar-link">一、卷积相关知识点</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_1-卷积-convolution" class="sidebar-link">1. 卷积（Convolution）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_2-填充-padding" class="sidebar-link">2. 填充（Padding）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_3-步幅-stride" class="sidebar-link">3. 步幅（Stride）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_4-神经网络中的张量表示" class="sidebar-link">4. 神经网络中的张量表示</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_5-池化-pooling" class="sidebar-link">5. 池化（Pooling）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_04/#_6-卷积与池化的输出尺寸计算" class="sidebar-link">6. 卷积与池化的输出尺寸计算</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/NN_04/#二、lenet-网络结构" class="sidebar-link">二、LeNet 网络结构</a></li></ul></li><li><a href="/pages/NN_05/" class="sidebar-link">NN 05.AlexNet 与使用“块”的神经网络</a></li><li><a href="/pages/NN_06/" class="sidebar-link">NN 06.并行块和深度神经网络的深究解释</a></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><span data-v-06225672>Neural Networks</span></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/liehuf/" target="_blank" title="作者" class="beLink" data-v-06225672>猎户f</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2026-02-01</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABH1JREFUSA3tVl1oHFUUPmdmd2ltklqbpJDiNnXFmgbFktho7YMPNiJSSZM0+CAYSkUELVhM6YuwIPpgoOKDqOBDC0XE2CQoNtQXBUFTTcCi+Wlh1V2TQExsUzcltd3M9Tt3ZjZzZ2fT+OJTL8yeM+eee757fmeJbq//KQL8X3DUSFOcfr7cRsRtxNQMWueeVzOkaITIGqQHNg5y8+jNW9ldM7A6nTpAjuolUikAwq7CE3WcM2RRDz+XGVgN3FptU/aUSlvq9Pa3iZ1+sgAqJyyAFqkipd9dqiwHF3P65YycLWc/6sqGrvoEoIp6DOFaX5h6+dnfjkWprwqsPk0dUGq5vySwDImC10KxFHgGL1SWoc92O3eVht09qdXNH11I2SsTsJYqMWzihqGMi+A+Garf3BAuuLI5oGlULyNfyB/HYNujwktOfRrMr5t77NmevqaUopx0grnKAyvVpmwUDB4x6FPXuGvYLTDwWsejwgtgkYKPqRJg8SV6xaiZ3ZTppGneS4yfH5/66fZSDHv+QZci/+h5c5UHtpy67JUqGppM0sh0Nc1dW6/N1W5Yoqat8/TU/VnadmdeW2PLLSyh0cvxBs3KbqTmwYPpxN4do/mzE8nEpvX/UMu2Wbp74zUAK5q6WkHns7V0eWkdPbPzd3rxkTGybadYySumVzhcaJFbs5UrEkQ/+CK8gF5dnh/6ciIZ73gwQ927L1IitoxKLXYP3SjYdOrHHfTZhRRlFyrorafPk20B3HPD1y2G3qKZME5Jcf3t/HUC13/8tSd++vqFveMUTwAUxSUFI1QekR1+bIze3D9MF2aq6cPvG72CgnldWCFqyRw3lwH8ZMerjTD9ElRO7Gv44wNpC90aASqGfVlz/Rx17srQ57/UU26hkhQqUB7dBR71WmzQhHUnblGmVOEw0jhbV1n9OlXUDCIRGaNV5Jp43N516fN7JmnTHdfp7Hgy0luO4aMhtkLL8Bi3bUWYvzh5Mn1dTxrL6QmGuRhGL/TiTTxRoEdTszSaq9GR0NGA3KdkOz3hqSV3MIDhQ5IVX/Ivx3umBti2es2h4eZby7x8br1rkf7Mo90AqC8aQ3sJeNzqFRu+vSANAQe3PL7l0HGOAdwDCeZYvNKeoZp1Qfs6Aipndh86HmFRi0LAnEO47wsqM6cdfjh3jBPUzhZy7nvlUfFsamED1VQt6aISHVymXZ/B2aCtIG8AI8xfobj2d3en1wWVhOeHELKmLQ1s211s88comkv4UCwWyF787mJdYXtNfhKAXVqnKTq8QZvGAGGOfaTo5pGZ/PwbUCr5+DPr/1J92JNHr9aOl/F3iI5+O1nfybsGxoimvZ3ViWSluDITw3P37mypheDIPY0tw7+O/5ApbkYw+zpfaUVu32Pi98+defdUhEpZkRFq0aqyNh9FuL9hpYbEm6iwi0z2REd09ZmyENEbuhjDWzKvZXTqKYaBIr3tt5kuPtQBZFvEUwHt60vfCNu41XsksH9Ij1BMMz1Y0OOunHNShFIP5868g5zeXmuLwL9T4b6Q2+KejgAAAABJRU5ErkJggg==">NN 04.卷积和LeNet神经网络<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p>本章用于记录我在学习卷积神经网络（CNN）过程中，
对卷积相关概念以及初代经典 CNN —— <strong>LeNet</strong> 的结构理解。</p> <h2 id="一、卷积相关知识点"><a href="#一、卷积相关知识点" class="header-anchor">#</a> 一、卷积相关知识点</h2> <h3 id="_1-卷积-convolution"><a href="#_1-卷积-convolution" class="header-anchor">#</a> 1. 卷积（Convolution）</h3> <p>在深度学习中，卷积运算本质上更接近于<strong>互相关（cross-correlation）</strong>。</p> <p>可以形象地理解为：<br>
一个较小的卷积核（kernel）在一张较大的输入特征图上，从左到右、从上到下滑动，
在每一个位置与局部区域进行加权求和，从而生成新的<strong>特征图（feature map）</strong>。</p> <hr> <h3 id="_2-填充-padding"><a href="#_2-填充-padding" class="header-anchor">#</a> 2. 填充（Padding）</h3> <p>填充是指在输入特征图的边缘补充若干行或列的像素。</p> <p>其主要目的包括：</p> <ul><li>控制输出特征图的空间尺寸</li> <li>保留输入图像边缘的信息</li></ul> <p>通常用 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span></span> 表示填充的像素数。</p> <hr> <h3 id="_3-步幅-stride"><a href="#_3-步幅-stride" class="header-anchor">#</a> 3. 步幅（Stride）</h3> <p>步幅表示卷积核在输入特征图上<strong>每次滑动的像素数</strong>，通常记为 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span></span></span>。</p> <ul><li><span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">S = 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span>：逐像素滑动（最常见）</li> <li><span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">S &gt; 1</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.72243em;vertical-align:-0.0391em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span>：下采样效果，输出尺寸减小</li></ul> <hr> <h3 id="_4-神经网络中的张量表示"><a href="#_4-神经网络中的张量表示" class="header-anchor">#</a> 4. 神经网络中的张量表示</h3> <p>在卷积神经网络中，输入数据通常表示为一个 <strong>四维张量</strong>：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="monospace">(batch_size,</mtext><mtext></mtext><mtext mathvariant="monospace">channels,</mtext><mtext></mtext><mtext mathvariant="monospace">height,</mtext><mtext></mtext><mtext mathvariant="monospace">width)</mtext></mrow><annotation encoding="application/x-tex">
\texttt{(batch\_size,\ channels,\ height,\ width)}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.9166599999999999em;vertical-align:-0.22222em;"></span><span class="mord text"><span class="mord texttt">(batch_size, channels, height, width)</span></span></span></span></span></span></div></section><p>各个维度的含义如下：</p> <ul><li><strong>batch_size</strong>：批量大小，一次输入网络的样本数量</li> <li><strong>channels</strong>：通道数（如灰度图为 1，RGB 图像为 3）</li> <li><strong>height</strong>：特征图高度</li> <li><strong>width</strong>：特征图宽度</li></ul> <hr> <h3 id="_5-池化-pooling"><a href="#_5-池化-pooling" class="header-anchor">#</a> 5. 池化（Pooling）</h3> <p>池化操作与卷积类似，同样在局部区域内进行运算，但有以下特点：</p> <ul><li><strong>不改变通道数</strong></li> <li>主要作用是减小特征图的空间尺寸</li> <li>提升平移不变性，降低计算量</li></ul> <p>常见的池化方式包括平均池化（Avg Pooling）和最大池化（Max Pooling）。</p> <hr> <h3 id="_6-卷积与池化的输出尺寸计算"><a href="#_6-卷积与池化的输出尺寸计算" class="header-anchor">#</a> 6. 卷积与池化的输出尺寸计算</h3> <p>对于卷积或池化操作，其输出特征图的空间尺寸计算公式为：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>output_size</mtext><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>input_size</mtext><mo>−</mo><mi>K</mi><mo>+</mo><mn>2</mn><mi>P</mi></mrow><mi>S</mi></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">
\text{output\_size}
=
\left\lfloor
\frac{\text{input\_size} - K + 2P}{S}
\right\rfloor
+ 1
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.97786em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">output_size</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size3">⌊</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.38333em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6999999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">input_size</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size3">⌋</span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span></span></div></section><p>其中：</p> <ul><li><strong>input_size</strong>：输入特征图的高度或宽度</li> <li><strong>K</strong>：卷积核（或池化核）的尺寸</li> <li><strong>P</strong>：填充大小（padding）</li> <li><strong>S</strong>：步幅（stride）</li> <li><span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⌊</mo><mo>⋅</mo><mo stretchy="false">⌋</mo></mrow><annotation encoding="application/x-tex">\lfloor \cdot \rfloor</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">⌊</span><span class="mord">⋅</span><span class="mclose">⌋</span></span></span></span></span>：向下取整</li></ul> <hr> <h2 id="二、lenet-网络结构"><a href="#二、lenet-网络结构" class="header-anchor">#</a> 二、LeNet 网络结构</h2> <p>下面使用 PyTorch 代码的形式，对经典 CNN —— <strong>LeNet</strong> 的整体结构进行说明。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第一层卷积</span>
    <span class="token comment"># 输入通道数为 1（灰度图），输出通道数为 6</span>
    <span class="token comment"># padding=2，使输出特征图尺寸保持为 28×28</span>

    nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 激活函数</span>
    <span class="token comment"># 将卷积结果映射到 (0, 1) 区间</span>

    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 平均池化层</span>
    <span class="token comment"># 不改变通道数，仅将特征图宽高减半：28 -&gt; 14</span>

    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 第二层卷积</span>
    <span class="token comment"># 输入通道数为 6，输出通道数为 16</span>
    <span class="token comment"># 特征图尺寸：14 -&gt; 10</span>

    nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 激活函数</span>

    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 平均池化层</span>
    <span class="token comment"># 特征图尺寸：10 -&gt; 5</span>

    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 展平层</span>
    <span class="token comment"># 将张量从 (16, 5, 5) 展平成 (400,)</span>

    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">400</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 全连接层</span>
    <span class="token comment"># 输入维度 400，输出维度 120</span>

    nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 激活函数</span>

    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 全连接层</span>
    <span class="token comment"># 通道数：120 -&gt; 84</span>

    nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token comment"># 激活函数</span>

    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
    <span class="token comment"># 输出层</span>
    <span class="token comment"># 输出维度等于类别数</span>
<span class="token punctuation">)</span></code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br></div></div></div></div> <!----> <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2026/02/28, 14:48:22</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/NN_03/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">NN 03.若干激活函数的研究</div></a> <a href="/pages/NN_05/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">NN 05.AlexNet 与使用“块”的神经网络</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/NN_03/" class="prev">NN 03.若干激活函数的研究</a></span> <span class="next"><a href="/pages/NN_05/">NN 05.AlexNet 与使用“块”的神经网络</a>→
      </span></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/liehuf" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:17715076182@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://blog.csdn.net/liehuf" title="CSDN" target="_blank" class="iconfont icon-csdn"></a><a href="https://www.zhihu.com/people/44-97-46-49" title="知乎" target="_blank" class="iconfont icon-zhihu"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2025-2026
    <span>liehuf | <a href="https://github.com/liehuf/liehuf-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.542ebbc9.js" defer></script><script src="/assets/js/2.e2bf2c87.js" defer></script><script src="/assets/js/15.350f58ce.js" defer></script>
  </body>
</html>
