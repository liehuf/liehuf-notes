<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>NN 06.并行块和深度神经网络的深究解释 | 原码纪事</title>
    <meta name="generator" content="VuePress 1.8.0">
    <link rel="icon" href="/img/base/favicon.ico">
    <script language="javascript" type="text/javascript" src="/js/pgmanor-self.js"></script>
    <script async="true" src="https://www.googletagmanager.com/gtag/js?id=G-LPRG9SPLFF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-LPRG9SPLFF');
    </script>
    <meta name="description" content="vdoing博客主题模板">
    <meta name="keywords" content="猎户f,golang,vue,go-web,go-admin,go-ldap-admin">
    <meta name="theme-color" content="#11a8cd">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <link rel="preload" href="/assets/css/0.styles.e44029f7.css" as="style"><link rel="preload" href="/assets/js/app.3261fe1d.js" as="script"><link rel="preload" href="/assets/js/2.e2bf2c87.js" as="script"><link rel="preload" href="/assets/js/17.094279c5.js" as="script"><link rel="prefetch" href="/assets/js/10.0302174d.js"><link rel="prefetch" href="/assets/js/11.107b05d0.js"><link rel="prefetch" href="/assets/js/12.6dd44963.js"><link rel="prefetch" href="/assets/js/13.7a68003b.js"><link rel="prefetch" href="/assets/js/14.4b1f4fe3.js"><link rel="prefetch" href="/assets/js/15.350f58ce.js"><link rel="prefetch" href="/assets/js/16.05e6aae6.js"><link rel="prefetch" href="/assets/js/18.8b6adf25.js"><link rel="prefetch" href="/assets/js/19.032901b0.js"><link rel="prefetch" href="/assets/js/20.c0302052.js"><link rel="prefetch" href="/assets/js/21.393603cc.js"><link rel="prefetch" href="/assets/js/22.7e530825.js"><link rel="prefetch" href="/assets/js/23.624f4e92.js"><link rel="prefetch" href="/assets/js/24.a83988fa.js"><link rel="prefetch" href="/assets/js/25.103ba1c8.js"><link rel="prefetch" href="/assets/js/26.7870e1c7.js"><link rel="prefetch" href="/assets/js/27.194d80a6.js"><link rel="prefetch" href="/assets/js/28.0a839261.js"><link rel="prefetch" href="/assets/js/3.8d869a92.js"><link rel="prefetch" href="/assets/js/4.a8f5cda8.js"><link rel="prefetch" href="/assets/js/5.274964a2.js"><link rel="prefetch" href="/assets/js/6.ba13907c.js"><link rel="prefetch" href="/assets/js/7.0701bfd8.js"><link rel="prefetch" href="/assets/js/8.5e498235.js"><link rel="prefetch" href="/assets/js/9.9c0da181.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e44029f7.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/base/bar.png" alt="原码纪事" class="logo"> <span class="site-name can-hide">原码纪事</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/pages/Notes/" class="nav-link">博客笔记</a></div><div class="nav-item"><a href="/pages/Projects/" class="nav-link">工程</a></div><div class="nav-item"><a href="/pages/Teasting/" class="nav-link">吐槽</a></div><div class="nav-item"><a href="/message-board/" class="nav-link">留言板</a></div><div class="nav-item"><a href="https://blog.yuanmajishi.top/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  我的博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><!----> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/pages/Notes/" class="nav-link">博客笔记</a></div><div class="nav-item"><a href="/pages/Projects/" class="nav-link">工程</a></div><div class="nav-item"><a href="/pages/Teasting/" class="nav-link">吐槽</a></div><div class="nav-item"><a href="/message-board/" class="nav-link">留言板</a></div><div class="nav-item"><a href="https://blog.yuanmajishi.top/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  我的博客
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/pages/NN_01/" class="sidebar-link">NN 01.关于线性网络的研究和记录</a></li><li><a href="/pages/NN_02/" class="sidebar-link">NN 02.若干损失函数的研究</a></li><li><a href="/pages/NN_03/" class="sidebar-link">NN 03.若干激活函数的研究</a></li><li><a href="/pages/NN_04/" class="sidebar-link">NN 04.卷积和LeNet神经网络</a></li><li><a href="/pages/NN_05/" class="sidebar-link">NN 05.AlexNet 与使用“块”的神经网络</a></li><li><a href="/pages/NN_06/" aria-current="page" class="active sidebar-link">NN 06.并行块和深度神经网络的深究解释</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/NN_06/#一、googlenet-网络并行化的阐述" class="sidebar-link">一、GoogLeNet 网络并行化的阐述</a></li><li class="sidebar-sub-header level2"><a href="/pages/NN_06/#二、批量规范化-batch-normalization" class="sidebar-link">二、批量规范化（Batch Normalization）</a></li><li class="sidebar-sub-header level2"><a href="/pages/NN_06/#三、resnet-的基本思想" class="sidebar-link">三、ResNet 的基本思想</a></li><li class="sidebar-sub-header level2"><a href="/pages/NN_06/#四、densenet-的研究解释" class="sidebar-link">四、DenseNet 的研究解释</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/NN_06/#稠密块-dense-block" class="sidebar-link">稠密块（Dense Block）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_06/#过渡层-transition-layer" class="sidebar-link">过渡层（Transition Layer）</a></li><li class="sidebar-sub-header level3"><a href="/pages/NN_06/#densenet-整体架构" class="sidebar-link">DenseNet 整体架构</a></li></ul></li></ul></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><span data-v-06225672>Neural Networks</span></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/liehuf/" target="_blank" title="作者" class="beLink" data-v-06225672>猎户f</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2026-02-01</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABGpJREFUSA3tVVtoXFUU3fvOI53UlmCaKIFmwEhsE7QK0ipFEdHEKpXaZGrp15SINsXUWvBDpBgQRKi0+KKoFeJHfZA+ED9KKoIU2gYD9UejTW4rVIzm0VSTziPzuNu1z507dibTTjL4U/DAzLn3nL3X2o91ziX6f9wMFdh6Jvbm9nNSV0msViVO6tN1Rm7NMu2OpeJ9lWBUTDxrJbYTS0hInuwciu9eLHlFxCLCZEk3MegsJmZ5K/JD6t7FkFdEvGUo1g7qJoG3MHImqRIn8/nzY1K9UPKKiJmtnUqHVE3Gbuay6vJE/N2FEmuxFjW2nUuE0yQXRRxLiTUAzs36zhZvOXJPdX850EVnnLZkB8prodQoM5JGj7Xk2mvC7JB8tG04Ef5PiXtG0UtxupRQSfTnBoCy554x18yJHI6I+G5Eru4LHmPJZEQsrvPUbMiA8G/WgMK7w7I+ez7++o2ANfbrjvaOl1tFMs+htG3IrZH9/hDX1Pr8Tc0UvH8tcX29KzAgIGcEkINyW5BF9x891hw6VYqgJHEk0huccS7vh3C6gTiODL+26huuBtbct8eZnqLML8PkxGYpuPZBqtqwkSjgc4mB5gbgig5i+y0UDK35LMxXisn9xQtK+nd26gTIHsHe/oblK/b29fUmN/8Y+9jAQrnBp56m1LcDlDp9irKTExSKduXJVWSqdBMA08pEJnEIOB3FPPMybu/oeV8zFeYN3xx576Q6RH+VmplE4ncQV5v+5rzSoyOU7PuEAg8g803PwBJ0CExno/jcMbN8tONYeOmHiuUNryvm3fRUy4tMPVLdAGkUhNWuggGrJcXPv+ouCjz0MKUHz1J2/E8IC9nqTabcxgaBYM0hPhD5Y65FsbxRQKxCQrDjDctW7PUM3HuZunFyifSAqEfuzCp48Il24luWUWZoyJCaPR82jE0+kFA643wRFVni4RYSq3ohJO2pZ7B5dO4xkDWbEpossJPLSrPjYID8rS2UHTlvyNxqIGsg674XJJ7vnh5L7PNwC4hh2sjCI96mzszOTpxLF0T7l88Yz7lAuK6OnL8gXLOnTvpzSb22YG8W7us3jSebFHeeqnXRG1vt+MoUM84LQIBmMsCTAcOauTh0T0l0neQK7m2bLMt2mGxU3HYssS0J2cdv5wljlPsrIuZLAG/2DOZIXgCYT8uMGZN+e2kSirfxZOPCsC0f24nTZzspnVn9VePS1Z5vubmAGGXG8ZFno9Hel0yfA5ZPhF7Dh972BQJ2qCpgH67lmWtBYbvk6sz02wjky2vXyz0XErP/kFB619js1BtwfOV4OPRqOQBjy3Qbk18vigUPPSD5ceHnwck7W9bhAqZdd7SuG7w4/P2F/GaJh8c7e9qgow+Q7cGBo+98WsLkuktFqiZabtXuQTu/Y5ETbR0v7tNSFnvrmu6pjdoan2KjMu8q/Hmj1EfCO2ZGfEIbIXKUlw8qaX9/b2oeSJmFksSeT/Fn0V3nSypChh4Gjh74ybO9aeZ/AN2dwciu2/MhAAAAAElFTkSuQmCC">NN 06.并行块和深度神经网络的深究解释<!----></h1> <!----> <div class="theme-vdoing-content content__default"><p>本篇博客着重于解释自己对于 GoogLeNet 以及深层网络 ResNet、DenseNet 的理解。</p> <h2 id="一、googlenet-网络并行化的阐述"><a href="#一、googlenet-网络并行化的阐述" class="header-anchor">#</a> 一、GoogLeNet 网络并行化的阐述</h2> <p>GoogLeNet 是一个巨大的进展，它的设计思想是从单纯追求网络深度，转向同时强调网络的宽度。</p> <p>一个 GoogLeNet 的 Inception 块由四条并行线路组成，它们共享相同的输入通道数，并在输出端进行通道维拼接（concatenate）。</p> <p>四条路径如下：</p> <p><strong>路径一：</strong></p> <p>Conv 1×1<br>
作用：保留空间分辨率，调整通道数，输出结果参与最终拼接。</p> <p><strong>路径二：</strong></p> <p>Conv 1×1（降维）<br>
→ Conv 3×3（padding=1）<br>
作用：在保证空间分辨率不变的前提下，引入中等感受野。</p> <p><strong>路径三：</strong></p> <p>Conv 1×1（降维）<br>
→ Conv 5×5（padding=2）<br>
作用：引入更大的感受野，捕捉更大尺度特征。</p> <p><strong>路径四：</strong></p> <p>MaxPool（kernel=3，stride=1，padding=1）<br>
→ Conv 1×1<br>
作用：引入池化特征，同时通过 1×1 卷积控制通道数。</p> <p>并行结构的核心意义在于：<br>
在同一层级同时观察不同尺度的特征，并将它们在通道维度上整合。</p> <h2 id="二、批量规范化-batch-normalization"><a href="#二、批量规范化-batch-normalization" class="header-anchor">#</a> 二、批量规范化（Batch Normalization）</h2> <p>批量规范化是一种在训练神经网络时用于稳定并加速训练过程的技术。</p> <p>在训练深层网络时，随着网络层数的加深，前面层参数的变化会不断改变后续层接收到的数据分布，这一现象被称为内部协变量转移（Internal Covariate Shift）。</p> <p>Batch Normalization 的目标是：<br>
使每一层的输入保持相对稳定的分布，避免激活值过大或过小，从而使梯度下降过程更加稳定、更易收敛。</p> <p>在每一个小批量（mini-batch）数据送入网络时，BatchNorm 对当前层的每个通道（或神经元）进行两步处理。</p> <p>第一步：对激活值做标准化（Normalization），计算当前 batch 的均值和标准差，将数据规范化为均值接近 0、标准差接近 1 的分布：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><mi>σ</mi></mfrac></mrow><annotation encoding="application/x-tex">
\hat{x} = \frac{x - \mu}{\sigma}
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.9463300000000001em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">σ</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div></section><p>第二步：引入可学习的缩放与平移参数：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>γ</mi><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">
y = \gamma \hat{x} + \beta
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span></span></div></section><p>其中 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span></span> 与 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span></span> 为可学习参数，使网络能够学习最合适的尺度与偏移，而非被固定为标准正态分布。</p> <p>Batch Normalization 的意义在于：<br>
加快训练速度（学习率可以调大）、降低对权重初始化的敏感性，并在一定程度上起到正则化作用（均值与方差存在噪声）。</p> <p>在使用习惯上，BatchNorm 一般放在激活函数之前；其参数 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05556em;">γ</span></span></span></span></span>、<span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span></span> 会参与反向传播；训练模式下使用当前 batch 的均值与方差，推理模式下使用全局移动平均估计值。</p> <h2 id="三、resnet-的基本思想"><a href="#三、resnet-的基本思想" class="header-anchor">#</a> 三、ResNet 的基本思想</h2> <p>ResNet 的核心思想是残差学习，即让网络块学习输入和输出之间的“差值”，而不是直接学习一个新的映射。</p> <p>其基本形式为：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
Y = X + F(X)
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span></span></div></section><p>一个基本残差块可描述为：</p> <div class="language-text line-numbers-mode"><pre class="language-text"><code>X
│
├─ Conv 3×3 → BN → ReLU → Conv 3×3 → BN ──┐
│                                         │
└──────────── Conv 1×1（可选）────────────┘
                      ↓
                   相加
                      ↓
                    ReLU
                      ↓
                      Y

</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>当输入与输出的通道数不一致时，可在捷径分支上使用 Conv 1×1 进行匹配。</p> <p>残差结构的核心作用在于缓解深层网络的退化问题，使梯度更容易在深层网络中传播。</p> <h2 id="四、densenet-的研究解释"><a href="#四、densenet-的研究解释" class="header-anchor">#</a> 四、DenseNet 的研究解释</h2> <p>DenseNet 的设计思想是：每一层都直接连接到之前所有层的特征图（feature map）。</p> <p>DenseNet 使用的是通道维拼接（concatenate），而非 ResNet 中的逐元素相加。</p> <p>其形式可表示为：</p> <section><div class="vuepress-eqn"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>X</mi><mo separator="true">,</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mi>X</mi><mo separator="true">,</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mn>3</mn></msub><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mi>X</mi><mo separator="true">,</mo><msub><mi>f</mi><mn>1</mn></msub><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>f</mi><mn>2</mn></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">
[X, f_1(X), f_2([X, f_1(X)]), f_3([X, f_1(X), f_2(\cdot)])]
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">]</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mopen">[</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span><span class="mclose">]</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></div></section><h3 id="稠密块-dense-block"><a href="#稠密块-dense-block" class="header-anchor">#</a> 稠密块（Dense Block）</h3> <p>一个最小计算单元由以下结构组成：</p> <p>BN → ReLU → Conv 3×3（padding=1）</p> <p>其输出通道数固定。</p> <p>核心计算逻辑如下：</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_convs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    layer<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
        conv_block<span class="token punctuation">(</span>num_channels <span class="token operator">*</span> i <span class="token operator">+</span> input_channels<span class="token punctuation">,</span> num_channels<span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>其中，<code>num_channels</code> 为增长率（growth rate）。
第 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span></span> 个卷积块的输入通道数等于原始输入通道数加上前 <span class="vuepress-eq"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span></span> 个卷积块所产生的新通道数。</p> <p>即：前面所有层的输出，都会作为后面所有层的输入。</p> <h3 id="过渡层-transition-layer"><a href="#过渡层-transition-layer" class="header-anchor">#</a> 过渡层（Transition Layer）</h3> <p>由于稠密连接会导致通道数快速增长，从而引发计算量和参数规模的膨胀，因此需要在稠密块之间引入过渡层进行控制。</p> <p>过渡层结构为：</p> <p>BN → ReLU → Conv 1×1 → AvgPool2d（stride=2）</p> <p>其作用是压缩通道数并减小特征图的空间尺寸。</p> <h3 id="densenet-整体架构"><a href="#densenet-整体架构" class="header-anchor">#</a> DenseNet 整体架构</h3> <p>初始层：Conv → BN → ReLU → MaxPool，用于提取初始特征并降低分辨率。</p> <p>稠密模块（Dense Block）：多层稠密连接，持续积累特征表示。</p> <p>过渡层（Transition Layer）：控制通道数量并减小特征图尺寸。</p> <p>输出阶段（分类部分）：BN → ReLU → 全局平均池化 → 全连接层 → 分类输出。</p></div></div> <!----> <div class="page-edit"><!----> <!----> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2026/02/08, 13:54:01</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/NN_05/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">NN 05.AlexNet 与使用“块”的神经网络</div></a> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/NN_05/" class="prev">NN 05.AlexNet 与使用“块”的神经网络</a></span> <!----></p></div></div></div> <!----></main></div> <div class="footer"><div class="icons"><a href="https://github.com/liehuf" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="mailto:17715076182@163.com" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://blog.csdn.net/liehuf" title="CSDN" target="_blank" class="iconfont icon-csdn"></a><a href="https://www.zhihu.com/people/44-97-46-49" title="知乎" target="_blank" class="iconfont icon-zhihu"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2025-2026
    <span>liehuf | <a href="https://github.com/liehuf/liehuf-notes/blob/main/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.3261fe1d.js" defer></script><script src="/assets/js/2.e2bf2c87.js" defer></script><script src="/assets/js/17.094279c5.js" defer></script>
  </body>
</html>
